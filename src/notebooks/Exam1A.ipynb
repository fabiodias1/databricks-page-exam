{"cells":[{"cell_type":"markdown","source":["# Simulado 1 - versão A"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b0ff9e3-b3d7-4513-832e-3ded97f60936"}}},{"cell_type":"markdown","source":["Obtendo dados do simulado"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540d3a10-ad93-43be-9ad7-8bb6a1b171d0"}}},{"cell_type":"code","source":["%run ./include/exam"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a0b03aa-7f75-438b-923b-539d5cd6dffd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["obj_Exam = Exam()\n\nobj_Exam.loadQuestionsData()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad8b8483-5c3e-440c-8065-6334ed5e3cbc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["lst_user_answers = [ \"\" for i in range(60)]\n\nobj_Exam.startTimeExam()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33e95a53-97c7-4166-b3e8-11116b9d1813"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 1\nWhich of the following statements about the Sparkdriver is incorrect?  \nA. The Spark driver is the node in which the Spark application's main method runs to\ncoordinate the Spark application.  \nB. The Spark driver is horizontally scaled to increase overall processing throughput.  \nC. The Spark driver contains the SparkContext object.   \nD. The Spark driver is responsible for scheduling the execution of data by various worker\nnodes in cluster mode.  \nE. The Spark driver should be as close as possible toworker nodes for optimal performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cccc2a9b-d217-42f8-8654-c03e4f77395d"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[0] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3438247d-742e-4364-9bb7-64eed2c9c9b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 2\nWhich of the following describes nodes in cluster-mode Spark?  \nA.Nodes are the most granular level of execution in the Spark execution hierarchy.  \nB.There is only one node and it hosts both the driver and executors.  \nC.Nodes are another term for executors, so they are processing engine instances for performing computations.  \nD.There are driver nodes and worker nodes, both of which can scale horizontally.  \nE.Worker nodes are machines that host the executors responsible for the execution of tasks."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a440245d-a16d-467d-a310-9962b1a85dd9"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[1] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20c2e200-7973-48c2-9bc0-4b0d2db79901"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 3\nWhich of the following statements about slots is true?  \nA.There must be more slots than executors.  \nB.There must be more tasks than slots.  \nC.Slots are the most granular level of execution inthe Spark execution hierarchy.  \nD.Slots are not used in cluster mode.  \nE.Slots are resources for parallelization within a Sparkapplication."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00af0de9-7f15-4bb5-bdcb-cc490086fd50"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[2] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae3a9851-6d5d-4971-ae7a-660b0ff301c2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 4\nWhich of the following is a combination of a block of data and a set of transformers that will run on a single executor?   \nA.Executor  \nB.Node  \nC.Job  \nD.Task  \nE.Slot"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d874d11-d348-4527-b02a-6b0678b23aaa"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[3] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1428c561-4ffc-4e9d-8be2-cff8d994ee66"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 5  \nWhich of the following is a group of tasks that can be executed in parallel to compute the same set  \nof operations on potentially multiple machines?  \nA.Job  \nB.Slot  \nC.Executor  \nD.Task  \nE.Stage"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e859e40f-702f-4e2f-a38c-fe27e7f9278f"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[4] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0bc12bb-3391-4e11-a276-9b71056d2cce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 6  \nWhich of the following describes a shuffle?  \nA. A shuffle is the process by which data is compared across partitions.  \nB. A shuffle is the process by which data is compared across executors.  \nC. A shuffle is the process by which partitions are allocated to tasks.  \nD. A shuffle is the process by which partitions are ordered for write.  \nE. A shuffle is the process by which tasks are ordered for execution."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04bd04cc-7d38-4ed5-bd2a-613ac4e1e307"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[5] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d91ab3af-5943-47ce-be46-bf09a38e0f0b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 7  \nDataFrame df is very large with a large number of partitions, more than there are executors in the  \ncluster. Based on this situation, which of the following is incorrect? Assume there is one core per  \nexecutor.  \nA. Performance will be suboptimal because not all executors will be utilized at the same time.  \nB. Performance will be suboptimal because not all data can be processed at the same time.  \nC. There will be a large number of shuffle connections performed on DataFrame df when  \noperations inducing a shuffle are called.  \nD. There will be a lot of overhead associated with managing resources for data processing  \nwithin each task.  \nE. There might be risk of out-of-memory errors depending on the size of the executors in the  \ncluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e71cb9dd-2b72-4a28-b496-51145e7be6a9"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[6] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6c482b7-2225-427f-b189-49056f9235fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 8  \nWhich of the following operations will trigger evaluation?  \nA. DataFrame.filter()  \nB. DataFrame.distinct()  \nC. DataFrame.intersect()  \nD. DataFrame.join()  \nE. DataFrame.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6f28a28-ce36-44f0-ae33-04376348bbc4"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[7] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a3395af-1383-4f96-9a59-3020bd914ce8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 9  \nWhich of the following describes the difference between transformations and actions?  \nA. Transformations work on DataFrames/Datasets while actions are reserved for native  \nlanguage objects.  \nB. There is no difference between actions and transformations.  \nC. Actions are business logic operations that do not induce execution while transformations  \nare execution triggers focused on returning results.  \nD. Actions work on DataFrames/Datasets while transformations are reserved for native  \nlanguage objects.  \nE. Transformations are business logic operations that do not induce execution while actions  \nare execution triggers focused on returning results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d965afb9-18bc-429e-94e6-769371269d53"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[8] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f6532a0-12d9-4307-b284-8c42219468ff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 10  \nWhich of the following DataFrame operations is always classified as a narrow transformation?  \nA. DataFrame.sort()  \nB. DataFrame.distinct()  \nC. DataFrame.repartition()  \nD. DataFrame.select()  \nE. DataFrame.join()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3934b615-a496-48fe-8da5-faae1bcb10a4"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[9] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b99c18fb-f8ac-4386-b0d5-1d0494bd5c93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 11  \nSpark has a few different execution/deployment modes: cluster, client, and local. Which of the  \nfollowing describes Spark's execution/deployment mode?  \nA. Spark's execution/deployment mode determines where the driver and executors are  \nphysically located when a Spark application is run  \nB. Spark's execution/deployment mode determines which tasks are allocated to which  \nexecutors in a cluster  \nC. Spark's execution/deployment mode determines which node in a cluster of nodes is  \nresponsible for running the driver program  \nD. Spark's execution/deployment mode determines exactly how many nodes the driver will  \nconnect to when a Spark application is run  \nE. Spark's execution/deployment mode determines whether results are run interactively in a  \nnotebook environment or in batch"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43369451-a96d-444a-a361-31b829d230a5"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[10] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ea8beb5-ba66-4306-a9f7-ca81520388ba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 12  \nWhich of the following cluster configurations will ensure the completion of a Spark application in  \nlight of a worker node failure?  \n  \nScenario #1  \n1 Driver & 1 Executor  \n- 100 GB and 200 Core per Executor  \n* Fixed cluster with one node  \n  \nScenario #4  \n1 Driver & 2 Executors  \n- 50 GB and 100 Core per Executor  \n* Fixed cluster with two nodes  \n  \nScenario #5  \n1 Driver & 4 Executor  \n- 25 GB and 50 Core per Executor  \n* Fixed cluster with 4 nodes  \n  \nScenario #6  \n1 Driver & 8 Executor  \n- 12.5 GB and 25 Core per Executor  \n* Fixed cluster with 8 nodes  \n  \nNote: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.  \nA. Scenario #1  \nB. They should all ensure completion because worker nodes are fault-tolerant.  \nC. Scenario #4  \nD. Scenario #5  \nE. Scenario #6"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"959b0848-0711-4476-b4f5-18c8e4ac29db"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[11] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d74321f-f3fd-434a-913d-ab85a974c8be"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 13  \nWhich of the following describes out-of-memory errors in Spark?  \nA. An out-of-memory error occurs when either the driver or an executor does not have enough  \nmemory to collect or process the data allocated to it.  \nB. An out-of-memory error occurs when Spark's storage level is too lenient and allows data  \nobjects to be cached to both memory and disk.  \nC. An out-of-memory error occurs when there are more tasks than are executors regardless of  \nthe number of worker nodes.  \nD. An out-of-memory error occurs when the Spark application calls too many transformations  \nin a row without calling an action regardless of the size of the data object on which the  \ntransformations are operating.  \nE. An out-of-memory error occurs when too much data is allocated to the driver for  \ncomputational purposes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a231cab9-8535-435c-8b50-bf4217d50668"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[12] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8293c677-0033-4950-b925-19030d270cd7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 14  \nWhich of the following is the default storage level for persist() for a non-streaming  \nDataFrame/Dataset?  \nA. MEMORY_AND_DISK  \nB. MEMORY_AND_DISK_SER  \nC. DISK_ONLY  \nD. MEMORY_ONLY_SER  \nE. MEMORY_ONLY"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"876357cd-bd35-49d7-b9fa-cd8d893f9a04"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[13] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1616f72-110d-4a29-95a8-164f7cebd255"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 15  \nWhich of the following describes a broadcast variable?  \nA. A broadcast variable is a Spark object that needs to be partitioned onto multiple worker  \nnodes because it's too large to fit on a single worker node.  \nB. A broadcast variable can only be created by an explicit call to the broadcast() operation.  \nC. A broadcast variable is entirely cached on the driver node so it doesn't need to be present  \non any worker nodes.  \nD. A broadcast variable is entirely cached on each worker node so it doesn't need to be  \nshipped or shuffled between nodes with each stage.  \nE. A broadcast variable is saved to the disk of each worker node to be easily read into memory  \nwhen needed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"846de428-bb88-4fd5-b203-e62fcd99a4de"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[14] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd3a0a6e-774e-475a-a693-a9c42b38da16"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 16  \nWhich of the following operations is most likely to induce a skew in the size of your data's  \npartitions?  \nA. DataFrame.collect()  \nB. DataFrame.cache()  \nC. DataFrame.repartition(n)  \nD. DataFrame.coalesce(n)  \nE. DataFrame.persist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"232ced3c-dd74-4246-8361-bb06af33b127"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[15] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dd340cc-a4cb-43bb-a70f-c5da3d66ac32"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 17  \nWhich of the following data structures are Spark DataFrames built on top of?  \nA. Arrays  \nB. Strings  \nC. RDDs  \nD. Vectors  \nE. SQL Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a697bce-dd33-4016-aa97-ee0773097dbb"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[16] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57109002-6c3e-41f5-b6a7-d3af4f84868b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 18  \nWhich of the following code blocks returns a DataFrame containing only column storeId and  \ncolumn division from DataFrame storesDF?  \nA. storesDF.select(\"storeId\").select(\"division\")  \nB. storesDF.select(storeId, division)  \nC. storesDF.select(\"storeId\", \"division\")  \nD. storesDF.select(col(\"storeId\", \"division\"))  \nE. storesDF.select(storeId).select(division)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f807cf1-df5e-4803-a730-2bca253f672f"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[17] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e50d308-ce48-4b9b-a83b-2b54bea0c657"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 19  \nWhich of the following code blocks returns a DataFrame containing all columns from DataFrame  \nstoresDF except for column sqft and column customerSatisfaction?  \nA sample of DataFrame storesDF is below:  \n\n|storeID|open |openDate  |division     |sqft |numberOfEmployees|customerSatisfaction|\n--------|-----|----------|-------------|-----|-----------------|---------------------\n|0      |true |1100746394|Utah         |43161|61               |71.1                |\n|1      |true |944572255 |West Virginia|18132|96               |43.46               |\n|2      |false|9254956288|West Virginia|79520|45               |36.93               |\n|3      |true |1397353092|Texas        |47751|78               |47.19               |\n|4      |true |986505057 |Delaware     |81483|95               |25.24               |\n|...    |...  |...       |...          |...  |...              |...                 |\n  \nA. storesDF.drop(\"sqft\", \"customerSatisfaction\")  \nB. storesDF.select(\"storeId\", \"open\", \"openDate\", \"division\")  \nC. storesDF.select(-col(sqft), -col(customerSatisfaction))  \nD. storesDF.drop(sqft, customerSatisfaction)  \nE. storesDF.drop(col(sqft), col(customerSatisfaction))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"008d9d07-a685-40d7-b1f1-98ce76a355a8"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[18] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd3d6851-a284-46c7-b6b3-54385b89ff2e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 20  \nThe below code shown block contains an error. The code block is intended to return a DataFrame  \ncontaining only the rows from DataFrame storesDF where the value in DataFrame storesDF's  \n\"sqft\" column is less than or equal to 25,000. Assume DataFrame storesDF is the only defined  \nlanguage variable. Identify the error.  \nCode block:  \n```\nstoresDF.filter(sqft <= 25000)  \n```\nA. The column name sqft needs to be quoted like storesDF.filter(\"sqft\" <= 25000).  \nB. The column name sqft needs to be quoted and wrapped in the col() function like storesDF.filter(col(\"sqft\") <= 25000).  \nC. The sign in the logical condition inside filter() needs to be changed from <= to >.  \nD. The sign in the logical condition inside filter() needs to be changed from <= to >=.  \nE. The column name sqft needs to be wrapped in the col() function like storesDF.filter(col(sqft) <= 25000)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f91d3c3-e90e-4841-bf01-b9248adc50a6"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[19] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8040a606-d269-438e-a62f-7c9f00bd7e39"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 21  \nThe code block shown below should return a DataFrame containing only the rows from DataFrame  \nstoresDF where the value in column sqft is less than or equal to 25,000 OR the value in column  \ncustomerSatisfaction is greater than or equal to 30. Choose the response that correctly fills  \nin the numbered blanks within the code block to complete this task.  \nCode block:  \n```\nstoresDF.__1__(__2__ __3__ __4__)  \n```\n\n```\nA.  \n  1. filter  \n  2. (col(\"sqft\") <= 25000)  \n  3. |  \n  4. (col(\"customerSatisfaction\") >= 30)  \nB.  \n  1. drop  \n  2. (col(sqft) <= 25000)  \n  3. |  \n  4. (col(customerSatisfaction) >= 30)  \nC.  \n  1. filter  \n  2. col(\"sqft\") <= 25000  \n  3. |  \n  4. col(\"customerSatisfaction\") >= 30  \nD.  \n  1. filter  \n  2. col(\"sqft\") <= 25000  \n  3. or  \n  4. col(\"customerSatisfaction\") >= 30  \nE.  \n  1. filter  \n  2. (col(\"sqft\") <= 25000)  \n  3. or  \n  4. (col(\"customerSatisfaction\") >= 30)  \n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aed65e9a-9611-450c-a74e-54e3ea09c580"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[20] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49112353-a28c-4831-acf5-68d10e13efb8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 22  \nWhich of the following operations can be used to convert a DataFrame column from one type to another type?  \nA. col().cast()  \nB. convert()  \nC. castAs()  \nD. col().coerce()  \nE. col()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e06f6cbb-20b9-4079-9b06-062998f8496d"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[21] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ab19ac7-cb99-4186-9e88-e1e6bc95e0d1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 23  \nWhich of the following code blocks returns a new DataFrame with a new column sqft100 that is  \n1/100th of column sqft in DataFrame storesDF? Note that column sqft100 is not in the original  \nDataFrame storesDF.  \nA. storesDF.withColumn(\"sqft100\", col(\"sqft\") * 100)  \nB. storesDF.withColumn(\"sqft100\", sqft / 100)  \nC. storesDF.withColumn(col(\"sqft100\"), col(\"sqft\") / 100)  \nD. storesDF.withColumn(\"sqft100\", col(\"sqft\") / 100)  \nE. storesDF.newColumn(\"sqft100\", sqft / 100)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d5681a2-9e17-4f7f-86e5-6d5258e1915d"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[22] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb9c45b8-c4ed-461f-bcea-3ef569cea62f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 24  \nWhich of the following code blocks returns a new DataFrame from DataFrame storesDF where  \ncolumn numberOfManagers is the constant integer 1?  \nA. storesDF.withColumn(\"numberOfManagers\", col(1))  \nB. storesDF.withColumn(\"numberOfManagers\", 1)  \nC. storesDF.withColumn(\"numberOfManagers\", lit(1))  \nD. storesDF.withColumn(\"numberOfManagers\", lit(\"1\"))  \nE. storesDF.withColumn(\"numberOfManagers\", IntegerType(1))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"507cfa8a-b494-4249-8c90-6f9acd48f0c0"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[23] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21100f25-9f11-4620-ad98-5855e3652dd1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 25  \nThe code block shown below contains an error. The code block intends to return a new DataFrame  \nwhere column storeCategory from DataFrame storesDF is split at the underscore character  \ninto column storeValueCategory and column storeSizeCategory. Identify the error.  \nA sample of DataFrame storesDF is displayed below:  \n\n|storeID|open |openDate  |storeCategory   |\n--------|-----|----------|-----------------\n|0      |true |1100746394|VALUE_MEDIUM    |\n|1      |true |944572255 |MAINSTREAM_SMALL|\n|2      |false|925495628 |PREMIUN_LARGE   |\n|3      |true |1397353092|VALUE_MEDIUM    |\n|4      |true |986505057 |VALUE_LARGE     |\n|5      |true |955988614 |PREMIUN_LARGE   |\n|...    |...  |...       |...             |\n  \nCode block:  \n```\n(storesDF.withColumn(  \n\t\t\"storeValueCategory\", col(\"storeCategory\").split(\"_\")[0]  \n\t).withColumn(  \n\t\t\"storeSizeCategory\", col(\"storeCategory\").split(\"_\")[1]  \n\t)  \n)  \n```  \nA. The split() operation comes from the imported functions object. It accepts a string  \ncolumn name and split character as arguments. It is not a method of a Column object.  \nB. The split() operation comes from the imported functions object. It accepts a Column  \nobject and split character as arguments. It is not a method of a Column object.  \nC. The index values of 0 and 1 should be provided as second arguments to the split()  \noperation rather than indexing the result.  \nD. The index values of 0 and 1 are not correct — they should be 1 and 2, respectively.  \nE. The withColumn() operation cannot be called twice in a row."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57214005-a929-479f-8c11-5c21ab667392"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[24] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5d26f9f-9597-483a-afc5-5adbdd539213"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 26  \nWhich of the following operations can be used to split an array column into an individual DataFrame  \nrow for each element in the array?  \nA. extract()  \nB. split()  \nC. explode()  \nD. arrays_zip()  \nE. unpack()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e27bf7ab-0edf-46a1-bd4b-6954b5077820"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[25] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee638fbf-9009-43b9-ac0a-4ca8b7098bcd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 27  \nWhich of the following code blocks returns a new DataFrame where column storeCategory is an  \nall-lowercase version of column storeCategory in DataFrame storesDF? Assume DataFrame  \nstoresDF is the only defined language variable.  \nA. storesDF.withColumn(\"storeCategory\",  \nlower(col(\"storeCategory\")))  \nB. storesDF.withColumn(\"storeCategory\",  \ncol(\"storeCategory\").lower())  \nC. storesDF.withColumn(\"storeCategory\",  \ntolower(col(\"storeCategory\")))  \nD. storesDF.withColumn(\"storeCategory\", lower(\"storeCategory\"))  \nE. storesDF.withColumn(\"storeCategory\", lower(storeCategory))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0ee2ee7-39e6-46eb-b7a2-18ff2ad6e22f"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[26] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"899b7992-ee18-45f8-9bb8-6410b5614941"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 28  \nThe code block shown below contains an error. The code block is intended to return a new  \nDataFrame where column division from DataFrame storesDF has been renamed to column  \nstate and column managerName from DataFrame storesDF has been renamed to column  \nmanagerFullName. Identify the error.  \nCode block:  \n```\n(storesDF.withColumnRenamed(\"state\", \"division\")  \n\t.withColumnRenamed(\"managerFullName\", \"managerName\"))  \n```  \nA. Both arguments to operation withColumnRenamed() should be wrapped in the col()  \noperation.  \nB. The operations withColumnRenamed() should not be called twice, and the first  \nargument should be [\"state\", \"division\"] and the second argument should be  \n[\"managerFullName\", \"managerName\"].  \nC. The old columns need to be explicitly dropped.  \nD. The first argument to operation withColumnRenamed() should be the old column name  \nand the second argument should be the new column name.  \nE. The operation withColumnRenamed() should be replaced with withColumn()."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f511ae1-9473-468f-839d-171dcfdf35bf"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[27] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c492a6f-93e6-4f1c-8a45-36ba1ce23d72"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 29  \nWhich of the following code blocks returns a DataFrame where rows in DataFrame storesDF  \ncontaining missing values in every column have been dropped?  \nA. storesDF.nadrop(\"all\")  \nB. storesDF.na.drop(\"all\", subset = \"sqft\")  \nC. storesDF.dropna()  \nD. storesDF.na.drop()  \nE. storesDF.na.drop(\"all\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cc2f29e-9b1f-40ca-87b1-754a9dbb8511"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[28] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f7a6c8e-7193-4aac-baff-0a75f5268ed4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 30  \nWhich of the following operations fails to return a DataFrame where every row is unique?  \nA. DataFrame.distinct()  \nB. DataFrame.drop_duplicates(subset = None)  \nC. DataFrame.drop_duplicates()  \nD. DataFrame.dropDuplicates()  \nE. DataFrame.drop_duplicates(subset = \"all\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd02cceb-ef7b-451a-b4f0-9484441ae50f"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[29] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b0638dd-d829-4934-8e3b-940069d70481"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 31  \nWhich of the following code blocks will not always return the exact number of distinct values in  \ncolumn division?  \nA. storesDF.agg(approx_count_distinct(col(\"division\")).alias(\"divi  \nsionDistinct\"))  \nB. storesDF.agg(approx_count_distinct(col(\"division\"),  \n0).alias(\"divisionDistinct\"))  \nC. storesDF.agg(countDistinct(col(\"division\")).alias(\"divisionDist  \ninct\"))  \nD. storesDF.select(\"division\").dropDuplicates().count()  \nE. storesDF.select(\"division\").distinct().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b6fca63-57b3-442a-9302-8f5a46edff27"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[30] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7691b099-3e00-4514-b229-e46c98c25d94"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 32  \nThe code block shown below should return a new DataFrame with the mean of column sqft from  \nDataFrame storesDF in column sqftMean. Choose the response that correctly fills in the  \nnumbered blanks within the code block to complete this task.  \nCode block:  \n```\nstoresDF.__1__(__2__(__3__).alias(\"sqftMean\"))  \nA.  \n\t1. agg  \n\t2. mean  \n\t3. col(\"sqft\")  \nB.  \n\t1. mean  \n\t2. col  \n\t3. \"sqft\"  \nC.  \n\t1. withColumn  \n\t2. mean  \n\t3. col(\"sqft\")  \nD.  \n\t1. agg  \n\t2. mean  \n\t3. \"sqft\"  \nE.  \n\t1. agg  \n\t2. average  \n\t3. col(\"sqft\")  \n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15c1297e-178a-49de-9d15-7e5c28725d0b"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[31] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0a9f692-0430-4ee3-9c3f-2c7ce9666c3b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 33  \nWhich of the following code blocks returns the number of rows in DataFrame storesDF?  \nA. storesDF.withColumn(\"numberOfRows\", count())  \nB. storesDF.withColumn(count().alias(\"numberOfRows\"))  \nC. storesDF.countDistinct()  \nD. storesDF.count()  \nE. storesDF.agg(count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dba18f8f-ff94-4ed9-962f-10cb452122b2"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[32] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61964307-a3de-420d-a9a0-868c8cbde2c5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 34  \nWhich of the following code blocks returns the sum of the values in column sqft in DataFrame storesDF grouped by distinct value in column division?  \nA. storesDF.groupBy.agg(sum(col(\"sqft\")))  \nB. storesDF.groupBy(\"division\").agg(sum())  \nC. storesDF.agg(groupBy(\"division\").sum(col(\"sqft\")))  \nD. storesDF.groupby.agg(sum(col(\"sqft\")))  \nE. storesDF.groupBy(\"division\").agg(sum(col(\"sqft\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2872b2f-8216-45aa-b399-59d6f849cac0"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[33] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47899dcb-858b-4b4f-b98d-4cdf8a43809c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 35  \nWhich of the following code blocks returns a DataFrame containing summary statistics only for  \ncolumn sqft in DataFrame storesDF?  \nA. storesDF.summary(\"mean\")  \nB. storesDF.describe(\"sqft\")  \nC. storesDF.summary(col(\"sqft\"))  \nD. storesDF.describeColumn(\"sqft\")  \nE. storesDF.summary()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8335c207-9751-4633-9bb9-3b44cf7648d9"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[34] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff3e1862-9e22-4b86-981b-109a58bab804"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 36  \nWhich of the following operations can be used to sort the rows of a DataFrame?  \nA. sort() and orderBy()  \nB. orderby()  \nC. sort() and orderby()  \nD. orderBy()  \nE. sort()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66db2451-aa8f-4648-9688-f1c1d2b914dc"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[35] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd9ddb6c-1b75-4ec7-b187-6d6bb54e5b4f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 37  \nThe code block shown below contains an error. The code block is intended to return a 15 percent  \nsample of rows from DataFrame storesDF without replacement. Identify the error.  \nCode block:  \n```\nstoresDF.sample(True, fraction = 0.15)  \n```\nA. There is no argument specified to the seed parameter.  \nB. There is no argument specified to the withReplacement parameter.  \nC. The sample() operation does not sample without replacement — sampleby() should be used instead.  \nD. The sample() operation is not reproducible.  \nE. The first argument True sets the sampling to be with replacement."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88879958-6d0e-48a8-ba8a-bf61809426a4"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[36] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d39f0de-0faa-4570-8414-57a5419f69a4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 38  \nWhich of the following operations can be used to return the top n rows from a DataFrame?  \nA. DataFrame.n()  \nB. DataFrame.take(n)  \nC. DataFrame.head  \nD. DataFrame.show(n)  \nE. DataFrame.collect(n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"562f13fe-3650-45a5-bbc3-346cabb33377"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[37] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"609c65a6-0e7b-4ec9-aec9-c24746dc2aa0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 39  \nThe code block shown below should extract the value for column sqft from the first row of  \nDataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the  \ncode block to complete this task.  \nCode block: \n``` \n__1__.__2__.__3__  \n\n```\n\n\nA.  \n\t1. storesDF  \n\t2. first  \n\t3. col(\"sqft\")  \nB.  \n\t1. storesDF  \n\t2. first  \n\t3. sqft  \nC.  \n\t1. storesDF  \n\t2. first  \n\t3. [\"sqft\"]  \nD.  \n\t1. storesDF  \n\t2. first()  \n\t3. sqft  \nE.  \n\t1. storesDF  \n\t2. first()  \n\t3. col(\"sqft\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a059c604-1bd6-4104-ae63-f9085acbb77d"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[38] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fcd29be-2a6f-4441-869d-f442c30cc55d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 40  \nWhich of the following lines of code prints the schema of a DataFrame?  \nA. print(storesDF)  \nB. storesDF.schema  \nC. print(storesDF.schema())  \nD. DataFrame.printSchema()  \nE. DataFrame.schema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32ab21ba-ee2d-41ce-ab30-e7fb844053e4"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[39] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f66a2125-0f83-4cef-82db-a5e2e56e59d8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 41  \nIn what order should the below lines of code be run in order to create and register a SQL UDF named  \n\"ASSESS_PERFORMANCE\" using the Python function assessPerformance and apply it to  \ncolumn customerSatistfaction in table stores?  \nLines of code:  \n1. spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)  \n2. spark.sql(\"SELECT customerSatisfaction,  assessPerformance(customerSatisfaction) AS result FROM stores\")  \n3. spark.udf.register(assessPerformance, \"ASSESS_PERFORMANCE\")  \n4. spark.sql(\"SELECT customerSatisfaction,  ASSESS_PERFORMANCE(customerSatisfaction) AS result FROM  stores\")  \n  \nA. 3, 4  \nB. 1, 4  \nC. 3, 2  \nD. 2  \nE. 1, 2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d2cb557-5782-4732-b037-13afda8d3239"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[40] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9faed14-4310-490d-92e0-15bb86228ce2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 42  \nIn what order should the below lines of code be run in order to create a Python UDF  \nassessPerformanceUDF() using the integer-returning Python function assessPerformance  \nand apply it to column customerSatisfaction in DataFrame storesDF?  \nLines of code:  \n1. assessPerformanceUDF = udf(assessPerformance, IntegerType)  \n2. assessPerformanceUDF = spark.register.udf(\"ASSESS_PERFORMANCE\",assessPerformance)  \n3. assessPerformanceUDF = udf(assessPerformance, IntegerType())  \n4. storesDF.withColumn(\"result\",assessPerformanceUDF(col(\"customerSatisfaction\")))  \n5. storesDF.withColumn(\"result\",assessPerformance(col(\"customerSatisfaction\")))  \n6. storesDF.withColumn(\"result\",ASSESS_PERFORMANCE(col(\"customerSatisfaction\")))  \nA. 3, 4  \nB. 2, 6  \nC. 3, 5  \nD. 1, 4  \nE. 2, 5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de9b972a-fc58-4566-9786-128863c674f1"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[41] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"461c6e46-0375-4a14-984d-0bb24cc7c25a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 43  \nWhich of the following operations can execute a SQL query on a table?  \nA. spark.query()  \nB. DataFrame.sql()  \nC. spark.sql()  \nD. DataFrame.createOrReplaceTempView()  \nE. DataFrame.createTempView()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d61a4fe-d898-47de-86f9-a5327cd095c9"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[42] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e70568f9-af5d-49aa-9d6b-b3857f376b58"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 44  \nWhich of the following code blocks creates a single-column DataFrame from Python list years  \nwhich is made up of integers?  \nA. spark.createDataFrame([years], IntegerType())  \nB. spark.createDataFrame(years, IntegerType())  \nC. spark.DataFrame(years, IntegerType())  \nD. spark.createDataFrame(years)  \nE. spark.createDataFrame(years, IntegerType)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e545b4b5-0b26-4771-8b46-6700fe3bbf3d"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[43] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72d9a491-6685-4629-bb67-b8e4c35afad6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 45  \nWhich of the following operations can be used to cache a DataFrame only in Spark’s memory  \nassuming the default arguments can be updated?  \nA. DataFrame.clearCache()  \nB. DataFrame.storageLevel  \nC. StorageLevel  \nD. DataFrame.persist()  \nE. DataFrame.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30c2fff2-be0f-423f-aa7a-5b6157054987"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[44] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b70e64ad-d5c9-4af5-af80-10af33fa3b26"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 46  \nThe code block shown below contains an error. The code block is intended to return a new  \n4-partition DataFrame from the 8-partition DataFrame storesDF without inducing a shuffle.  \nIdentify the error.  \nCode block:  \nstoresDF.repartition(4)  \nA. The repartition operation will only work if the DataFrame has been cached to memory.  \nB. The repartition operation requires a column on which to partition rather than a number  \nof partitions.  \nC. The number of resulting partitions, 4, is not achievable for an 8-partition DataFrame.  \nD. The repartition operation induced a full shuffle. The coalesce operation should be  \nused instead.  \nE. The repartition operation cannot guarantee the number of result partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b5d517d-c8d4-49a6-a126-4239cbc89287"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[45] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b90f0c6-7543-45ad-b982-90bc916403c7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 47  \nWhich of the following code blocks will always return a new 12-partition DataFrame from the  \n8-partition DataFrame storesDF?  \nA. storesDF.coalesce(12)  \nB. storesDF.repartition()  \nC. storesDF.repartition(12)  \nD. storesDF.coalesce()  \nE. storesDF.coalesce(12, \"storeId\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"126bfd80-9eda-47a8-886e-faa43b667fa1"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[46] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cb8e07a-2ff8-4d86-8ae0-9367ac324262"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 48  \nWhich of the following Spark config properties represents the number of partitions used in wide  \ntransformations like join()?  \nA. spark.sql.shuffle.partitions  \nB. spark.shuffle.partitions  \nC. spark.shuffle.io.maxRetries  \nD. spark.shuffle.file.buffer  \nE. spark.default.parallelism"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"503947fc-29fb-4e11-bedf-763e0e676415"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[47] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a21f53b1-9882-458f-adf1-2ce135b82c18"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 49  \nIn what order should the below lines of code be run in order to return a DataFrame containing a  \ncolumn openDateString, a string representation of Java’s SimpleDateFormat?  \nNote that column openDate is of type integer and represents a date in the UNIX epoch format —  \nthe number of seconds since midnight on January 1st, 1970.  \nAn example of Java's SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 PM\".  \nA sample of storesDF is displayed below:  \n\nstoreID|openDate  \n--------|-----------  \n0      |1100746394\n1      |1474410343\n2      |111661009 \n3      |1180035265\n4      |1408024997\n...    |...       \n  \nLines of code:  \n```\n1. storesDF.withColumn(\"openDateString\",  from_unixtime(col(\"openDate\"), simpleDateFormat))  \n2. simpleDateFormat = \"EEEE, MMM d, yyyy h:mm a\"  \n3. storesDF.withColumn(\"openDateString\",  from_unixtime(col(\"openDate\"), SimpleDateFormat()))  \n4. storesDF.withColumn(\"openDateString\",  date_format(col(\"openDate\"), simpleDateFormat))  \n5. storesDF.withColumn(\"openDateString\",  date_format(col(\"openDate\"), SimpleDateFormat()))  \n6. simpleDateFormat = \"wd, MMM d, yyyy h:mm a\" \n```\n   \nA. 2, 3  \nB. 2, 1  \nC. 6, 5  \nD. 2, 4  \nE. 6, 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f9f5f6f-a861-4f87-8fff-01519a589c78"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[48] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"860975ec-6ab0-42d3-bd02-5574feb01cc1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 50  \nWhich of the following code blocks returns a DataFrame containing a column month, an integer representation of the month from column openDate from DataFrame storesDF?  \nNote that column openDate is of type integer and represents a date in the UNIX epoch format — the number of seconds since midnight on January 1st, 1970.  \nA sample of storesDF is displayed below:  \n \nstoreID|openDate  \n-------|-----------\n0      |1100746394\n1      |1474410343\n2      |111661009 \n3      |1180035265\n4      |1408024997\n...    |...       \n  \nA. storesDF.withColumn(\"month\", getMonth(col(\"openDate\")))  \nB. storesDF.withColumn(\"openTimestamp\",  col(\"openDate\").cast(\"Timestamp\")).withColumn(\"month\",  month(col(\"openTimestamp\")))  \nC. storesDF.withColumn(\"openDateFormat\",  col(\"openDate\").cast(\"Date\")).withColumn(\"month\",  month(col(\"openDateFormat\")))  \nD. storesDF.withColumn(\"month\", substr(col(\"openDate\"), 4, 2))  \nE. storesDF.withColumn(\"month\", month(col(\"openDate\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4890b461-d172-4632-82fd-48e573df7585"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[49] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4e7bfc6-0383-4c41-ab4f-7f7f99f95e85"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 51  \nWhich of the following operations performs an inner join on two DataFrames?  \nA. DataFrame.innerJoin()  \nB. DataFrame.join()  \nC. Standalone join() function  \nD. DataFrame.merge()  \nE. DataFrame.crossJoin()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bf08ca6-3528-4995-959c-cdf96fe1bc95"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[50] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac2391e5-46a2-41e2-9b6a-43f02bd65944"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 52  \nWhich of the following code blocks returns a new DataFrame that is the result of an outer join between DataFrame storesDF and DataFrame employeesDF on column storeId?  \nA. storesDF.join(employeesDF, \"storeId\", \"outer\")  \nB. storesDF.join(employeesDF, \"storeId\")  \nC. storesDF.join(employeesDF, \"outer\", col(\"storeId\"))  \nD. storesDF.join(employeesDF, \"outer\", storesDF.storeId ==  employeesDF.storeId)  \nE. storesDF.merge(employeesDF, \"outer\", col(\"storeId\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e542f1c-23c6-4fe4-9395-369d8e354fd8"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[51] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc922d6b-67c8-4ed8-bfc1-1e8a51446ad5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 53  \nThe below code block contains an error. The code block is intended to return a new DataFrame that is the result of an inner join between DataFrame storesDF and DataFrame employeesDF on column storeId and column employeeId which are in both DataFrames. Identify the error.  \nCode block:  \n```\nstoresDF.join(employeesDF, [col(\"storeId\"), col(\"employeeId\")])  \n```\n\nA. The join() operation is a standalone function rather than a method of DataFrame — the join() operation should be called where its first two arguments are storesDF and employeesDF.  \nB. There must be a third argument to join() because the default to the how parameter is not \"inner\".  \nC. The col(\"storeId\") and col(\"employeeId\") arguments should not be separate elements of a list — they should be tested to see if they're equal to one another like col(\"storeId\") == col(\"employeeId\").  \nD. There is no DataFrame.join() operation — DataFrame.merge() should be used instead.  \nE. The references to \"storeId\" and \"employeeId\" should not be inside the col() function — removing the col() function should result in a successful join."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed2a2b18-d530-45aa-9ecf-56d03ea773d7"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[52] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16787b51-3942-4074-815c-46a088c7062b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 54  \nWhich of the following Spark properties is used to configure the broadcasting of a DataFrame  \nwithout the use of the broadcast() operation?  \nA. spark.sql.autoBroadcastJoinThreshold  \nB. spark.sql.broadcastTimeout  \nC. spark.broadcast.blockSize  \nD. spark.broadcast.compress  \nE. spark.executor.memoryOverhead"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07e7a785-5d8f-4623-9d14-985015456372"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[53] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41cf3450-518e-48e4-81ec-2ededa5c4d90"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 55  \nThe code block shown below should return a new DataFrame that is the result of a cross join  \nbetween DataFrame storesDF and DataFrame employeesDF. Choose the response that  \ncorrectly fills in the numbered blanks within the code block to complete this task.  \nCode block:  \n```\n__1__.__2__(__3__)  \n```\nA.  \n\t1. storesDF  \n\t2. crossJoin  \n\t3. employeesDF, \"storeId\"  \nB.  \n\t1. storesDF  \n\t2. join  \n\t3. employeesDF, \"cross\"  \nC.  \n\t1. storesDF  \n\t2. crossJoin  \n\t3. employeesDF, \"storeId\"  \nD.  \n\t1. storesDF  \n\t2. join  \n\t3. employeesDF, \"storeId\", \"cross\"  \nE.  \n\t1. storesDF  \n\t2. crossJoin  \n\t3. employeesDF"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c16dce9-759a-4475-824f-82c6092f8395"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[54] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff91e5c8-2593-419b-bbfc-3d267860c9ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 56  \nWhich of the following operations performs a position-wise union on two DataFrames?  \nA. The standalone concat() function  \nB. The standalone unionAll() function  \nC. The standalone union() function  \nD. DataFrame.unionByName()  \nE. DataFrame.union()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0a5aa1c-2ccb-4771-95f2-b50f06b4b229"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[55] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc817050-4dad-44c2-8a4e-113511e69f5d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 57  \nWhich of the following code blocks writes DataFrame storesDF to file path filePath as  \nparquet?  \nA. storesDF.write.option(\"parquet\").path(filePath)  \nB. storesDF.write.path(filePath)  \nC. storesDF.write().parquet(filePath)  \nD. storesDF.write(filePath)  \nE. storesDF.write.parquet(filePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e84bf3f5-9a11-4745-83c0-beafc1290091"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[56] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a5f97b2-23ed-4556-87b5-8ee54191d0f4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 58  \nThe code block shown below contains an error. The code block is intended to write DataFrame  \nstoresDF to file path filePath as parquet and partition by values in column division. Identify  \nthe error.  \nCode block:  \n```\nstoresDF.write.repartition(\"division\").parquet(filePath)  \n```\n\nA. The argument division to operation repartition() should be wrapped in the col() function to return a Column object.  \nB. There is no parquet() operation for DataFrameWriter — the save() operation should be used instead.  \nC. There is no repartition() operation for DataFrameWriter — the partitionBy() operation should be used instead.  \nD. DataFrame.write is an operation — it should be followed by parentheses to return a DataFrameWriter.  \nE. The mode() operation must be called to specify that this write should not overwrite existing files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"456b0731-8b1f-47e5-8d3a-8291b773e123"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[57] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6481cccd-c13b-400e-bc92-5c596d90d0fd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 59  \nWhich of the following code blocks reads a parquet at the file path filePath into a DataFrame?  \nA. spark.read().parquet(filePath)  \nB. spark.read().path(filePath, source = \"parquet\")  \nC. spark.read.path(filePath, source = \"parquet\")  \nD. spark.read.parquet(filePath)  \nE. spark.read().path(filePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97dce1a8-0006-4fe4-a85e-4d366b235f2b"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[58] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"124ebb29-0fa1-4d2d-a437-2e29f5da50d4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Question 60  \nWhich of the following code blocks reads JSON at the file path filePath into a DataFrame  \nwith the specified schema schema?  \nA. spark.read().schema(schema).format(json).load(filePath)  \nB. spark.read().schema(schema).format(\"json\").load(filePath)  \nC. spark.read.schema(\"schema\").format(\"json\").load(filePath)  \nD. spark.read.schema(\"schema\").format(\"json\").load(filePath)  \nE. spark.read.schema(schema).format(\"json\").load(filePath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68af2659-89ce-4efb-822e-a58558a32d1d"}}},{"cell_type":"code","source":["#Replace @ with the letter of the chose option.\nlst_user_answers[59] = \"@\"\n\nif obj_Exam.checkTimeOver():\n  print(\"Your time is over!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f94660a-a75f-4c60-98b9-959951ff8683"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["lst_answers = obj_Exam.getAnswers()\n\nint_hits = 0\n\nfor (userAnswer, sheetAnswer) in zip(lst_user_answers, lst_answers):\n  if userAnswer.upper() == sheetAnswer.upper():\n    int_hits += 1\n    \nprint(\"Você acertou\", int_hits, \"questões\")\n\nif int_hits >= 42:\n  print(\"Você foi aprovado.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d0cc82d-70f0-4787-89e4-cba9a35b9027"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Fim do simulado."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f3f841f-c400-4cd9-b5af-6093d69101ac"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Exam1A","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3472718223316963}},"nbformat":4,"nbformat_minor":0}
